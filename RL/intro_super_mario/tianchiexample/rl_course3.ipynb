{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R4 PPO（近段策略优化）算法讲解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1、策略（要输出最优动作的策略模型）\n",
    "\n",
    "2、近端（代理函数的剪裁）\n",
    "\n",
    "3、优化（使用代理函数）的出现及其实际意义，导致了算法的命名。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4.1 由浅入深，简化版ppo（100行代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入gym和torch相关包\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0005 #学习率\n",
    "gamma         = 0.98   #\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义PPO架构\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = [] #用来存储交互数据\n",
    "        \n",
    "        self.fc1   = nn.Linear(4,256) #由于倒立摆环境简单，这里仅用一个线性变换来训练数据\n",
    "        self.fc_pi = nn.Linear(256,2) #policy函数（输出action）的全连接层\n",
    "        self.fc_v  = nn.Linear(256,1) #value函数（输出v）的全连接层\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate) #优化器\n",
    "\n",
    "    #policy函数\n",
    "    #输入观测值x\n",
    "    #输出动作空间概率，从而选择最优action\n",
    "    def pi(self, x, softmax_dim = 0): \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    #value函数\n",
    "    #输入观测值x\n",
    "    #输出x状态下value的预测值（reward）,提供给policy函数作为参考值\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "    \n",
    "    #把交互数据存入buffer\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    #把数据形成batch，训练模型时需要一个一个batch输入模型\n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, done = transition\n",
    "            \n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                          torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "    \n",
    "    \n",
    "    #训练模型\n",
    "    \n",
    "    def train_net(self):\n",
    "        #make batch 数据，喂给模型\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch): #K_epoch：训练多少个epoch\n",
    "            #计算td_error 误差，value模型的优化目标就是尽量减少td_error\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "            delta = td_target - self.v(s)\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            #计算advantage:\n",
    "            #即当前策略比一般策略（baseline）要好多少\n",
    "            #policy的优化目标就是让当前策略比baseline尽量好，但是每次更新时又不能偏离太多，所以后面会有个clip\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            #计算ratio 防止单词更新偏离太多\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            #通过clip 保证ratio在（1-eps_clip, 1+eps_clip）范围内\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            #这里简化ppo，把policy loss和value loss放在一起计算\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
    "\n",
    "            #梯度优化\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主函数：简化ppo 这里先交互T_horizon个回合然后停下来学习训练，再交互，这样循环10000次\n",
    "def main():\n",
    "    #创建倒立摆环境\n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = PPO()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    #主循环\n",
    "    for n_epi in range(10000):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(T_horizon):\n",
    "                #由当前policy模型输出最优action\n",
    "                prob = model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                #用最优action进行交互\n",
    "                s_prime, r, done, info = env.step(a)\n",
    "\n",
    "                #存储交互数据，等待训练\n",
    "                model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n",
    "                s = s_prime\n",
    "\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            #模型训练\n",
    "            model.train_net()\n",
    "\n",
    "        #打印每轮的学习成绩\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4.2 openai版本ppo算法实践 （训练超级玛丽）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面简化版本的ppo玩倒立摆，你已经对ppo有了简单的认知，但是上面的做法还有很多待改进的地方，比如模型太简单，如果像超级玛丽这种观测值是图像的话，简单的线性变换肯定不满足条件\n",
    "、策略模型和value模型应该分开优化损失，因为policy和value的loss很多情况下数值是差别很大的，小的那个往往得不到有效优化等\n",
    "接下来介绍openai 官方版本的ppo怎么实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.然后我们来设计ppo算法来实现马里奥通关\n",
    "#3.1 先创建游戏环境（\n",
    "#    a.组合定义action\n",
    "#    b.重定义reward\n",
    "#    c.堆叠zhenlv\n",
    "#    d.预处理输入的图像\n",
    "#    ）\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关包\n",
    "import gym_super_mario_bros\n",
    "from gym.spaces import Box\n",
    "from gym import Wrapper\n",
    "from nes_py.wrappers import JoypadSpace#BinarySpaceToDiscreteSpaceEnv\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n",
    "import cv2\n",
    "import numpy as np\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monitor:\n",
    "    def __init__(self, width, height, saved_path):\n",
    "\n",
    "        self.command = [\"ffmpeg\", \"-y\", \"-f\", \"rawvideo\", \"-vcodec\", \"rawvideo\", \"-s\", \"{}X{}\".format(width, height),\n",
    "                        \"-pix_fmt\", \"rgb24\", \"-r\", \"80\", \"-i\", \"-\", \"-an\", \"-vcodec\", \"mpeg4\", saved_path]\n",
    "        try:\n",
    "            self.pipe = sp.Popen(self.command, stdin=sp.PIPE, stderr=sp.PIPE)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def record(self, image_array):\n",
    "        self.pipe.stdin.write(image_array.tostring())\n",
    "\n",
    "\n",
    "def process_frame(frame):\n",
    "    if frame is not None:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (84, 84))[None, :, :] / 255.\n",
    "        return frame\n",
    "    else:\n",
    "        return np.zeros((1, 84, 84))\n",
    "\n",
    "\n",
    "class CustomReward(Wrapper):\n",
    "    def __init__(self, env=None, monitor=None):\n",
    "        super(CustomReward, self).__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 84, 84))\n",
    "        self.curr_score = 0\n",
    "        if monitor:\n",
    "            self.monitor = monitor\n",
    "        else:\n",
    "            self.monitor = None\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        if self.monitor:\n",
    "            self.monitor.record(state)\n",
    "        state = process_frame(state)\n",
    "        reward += (info[\"score\"] - self.curr_score) / 40.\n",
    "        self.curr_score = info[\"score\"]\n",
    "        if done:\n",
    "            if info[\"flag_get\"]:\n",
    "                reward += 50\n",
    "            else:\n",
    "                reward -= 50\n",
    "        return state, reward / 10., done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_score = 0\n",
    "        return process_frame(self.env.reset())\n",
    "\n",
    "\n",
    "class CustomSkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super(CustomSkipFrame, self).__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(4, 84, 84))\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        states = []\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        for i in range(self.skip):\n",
    "            if not done:\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                states.append(state)\n",
    "            else:\n",
    "                states.append(state)\n",
    "        states = np.concatenate(states, 0)[None, :, :, :]\n",
    "        return states.astype(np.float32), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        states = np.concatenate([state for _ in range(self.skip)], 0)[None, :, :, :]\n",
    "        return states.astype(np.float32)\n",
    "\n",
    "\n",
    "def create_train_env(world, stage, action_type, output_path=None):\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-{}-{}-v0\".format(world, stage))\n",
    "    if output_path:\n",
    "        monitor = Monitor(256, 240, output_path)\n",
    "    else:\n",
    "        monitor = None\n",
    "    if action_type == \"right\":\n",
    "        actions = RIGHT_ONLY\n",
    "    elif action_type == \"simple\":\n",
    "        actions = SIMPLE_MOVEMENT\n",
    "    else:\n",
    "        actions = COMPLEX_MOVEMENT\n",
    "    env = JoypadSpace(env, actions)\n",
    "    env = CustomReward(env, monitor)\n",
    "    env = CustomSkipFrame(env)\n",
    "    return env, env.observation_space.shape[0], len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.创建ppo算法\n",
    "#5.1创建ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义通用cnn model类\n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, num_inputs, num_out, activation=nn.ReLU):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.lstm = nn.Linear(32 * 6 * 6, 512)\n",
    "        # self.critic_linear = nn.Linear(512, 1)\n",
    "        # self.actor_linear = nn.Linear(512, num_actions)\n",
    "        self.fc_out = nn.Linear(512, num_out)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # nn.init.kaiming_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.LSTMCell):\n",
    "                nn.init.constant_(module.bias_ih, 0)\n",
    "                nn.init.constant_(module.bias_hh, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.lstm(x))\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils:\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class userActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = cnn_model(obs_dim, act_dim, activation=activation)\n",
    "        print(self.logits_net)\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        pi = Categorical(logits=self.logits_net(obs))\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = pi.log_prob(act)\n",
    "        return pi, logp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class userCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = cnn_model(obs_dim, 1, activation=activation)#cnn_net([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "        print(self.v_net)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.3定义ppo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import scipy.signal\n",
    "# import spinup.algos.pytorch.ppo.core as core\n",
    "# from core_1 import Actor, Critic\n",
    "from core import userCritic, userActor\n",
    "from env import create_train_env\n",
    "from spinup.utils.logx import EpochLogger\n",
    "from spinup.utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "\n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        #data.to(device)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32).to(device) for k,v in data.items()}\n",
    "\n",
    "def ppo(env_fn, actor=nn.Module, critic=nn.Module, ac_kwargs=dict(), seed=0,\n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        target_kl=0.01, logger_kwargs=dict(), save_freq=10):\n",
    "    # Special function to avoid certain slowdowns from PyTorch + MPI combo.\n",
    "    setup_pytorch_for_mpi()\n",
    "\n",
    "    # Set up logger and save configuration\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    # Random seed\n",
    "    seed += 10000 * proc_id()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.n\n",
    "    \n",
    "    # Create actor-critic module\n",
    "    ac_pi = actor(obs_dim[0], act_dim, hidden_sizes=[64, 64], activation=nn.Tanh)  # env.observation_space, env.action_space, nn.ReLU)\n",
    "    ac_v = critic(obs_dim[0], hidden_sizes=[64, 64], activation=nn.Tanh)  # env.observation_space, nn.ReLU)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cup')\n",
    "    ac_pi.to(device)\n",
    "    ac_v.to(device)\n",
    "\n",
    "    # Sync params across processes\n",
    "    sync_params(ac_pi)\n",
    "    sync_params(ac_v)\n",
    "\n",
    "    # Count variables\n",
    "    def count_vars(module):\n",
    "        return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "    var_counts = tuple(count_vars(module) for module in [ac_pi, ac_v])\n",
    "    logger.log('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "    # Set up experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch / num_procs())\n",
    "    buf = PPOBuffer(obs_dim, env.action_space.shape, local_steps_per_epoch, gamma, lam)\n",
    "    \n",
    "    # Set up function for computing PPO policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac_pi(obs, act)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+clip_ratio) | ratio.lt(1-clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((ac_v(obs) - ret)**2).mean()\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = Adam(ac_pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(ac_v.parameters(), lr=vf_lr)\n",
    "\n",
    "    # Set up model saving\n",
    "    logger.setup_pytorch_saver(ac_pi)\n",
    "    \n",
    "    def update():\n",
    "        data = buf.get()\n",
    "\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = compute_loss_pi(data)\n",
    "            kl = mpi_avg(pi_info['kl'])\n",
    "            if kl > 1.5 * target_kl:\n",
    "                logger.log('Early stopping at step %d due to reaching max kl.'%i)\n",
    "                break\n",
    "            loss_pi.backward()\n",
    "            mpi_avg_grads(ac_pi)    # average grads across MPI processes\n",
    "            pi_optimizer.step()\n",
    "\n",
    "        logger.store(StopIter=i)\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            mpi_avg_grads(ac_v)    # average grads across MPI processes\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        # Log changes from update\n",
    "        kl, ent, cf = pi_info['kl'], pi_info_old['ent'], pi_info['cf']\n",
    "        logger.store(LossPi=pi_l_old, LossV=v_l_old,\n",
    "                     KL=kl, Entropy=ent, ClipFrac=cf,\n",
    "                     DeltaLossPi=(loss_pi.item() - pi_l_old),\n",
    "                     DeltaLossV=(loss_v.item() - v_l_old))\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            # a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "            with torch.no_grad():\n",
    "                rr = torch.from_numpy(o.copy()).float().to(device)#.unsqueeze(0)\n",
    "                pi, _ = ac_pi(rr, None)\n",
    "                a = pi.sample()\n",
    "                # logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "                logp = pi.log_prob(a)#.sum(axis=-1)\n",
    "                v = ac_v(torch.as_tensor(o, dtype=torch.float32).to(device))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a.cpu().numpy().item())\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a.cpu().numpy(), r, v.cpu().numpy(), logp.cpu().numpy())\n",
    "            logger.store(VVals=v.cpu().numpy())\n",
    "\n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d #or timeout\n",
    "            epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if epoch_ended:\n",
    "                    print('epoch_end')\n",
    "                    # _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                    with torch.no_grad():\n",
    "                        v =ac_v(torch.from_numpy(o).float().to(device)).cpu().numpy()\n",
    "                else:\n",
    "                    print('epret :',ep_ret)\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Perform PPO update!\n",
    "        update()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('VVals', with_min_and_max=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', (epoch+1)*steps_per_epoch)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossV', average_only=True)\n",
    "        logger.log_tabular('DeltaLossPi', average_only=True)\n",
    "        logger.log_tabular('DeltaLossV', average_only=True)\n",
    "        logger.log_tabular('Entropy', average_only=True)\n",
    "        logger.log_tabular('KL', average_only=True)\n",
    "        logger.log_tabular('ClipFrac', average_only=True)\n",
    "        logger.log_tabular('StopIter', average_only=True)\n",
    "        logger.log_tabular('Time', time.time()-start_time)\n",
    "        logger.dump_tabular()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    hid_sizes = 128\n",
    "    gamma = 0.999\n",
    "    seed = 0\n",
    "    steps = 10000\n",
    "    epochs = 150\n",
    "    cpu = 1\n",
    "    exp_name = \"ppo\"\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "    mpi_fork(cpu)  # run parallel code with mpi\n",
    "\n",
    "    from spinup.utils.run_utils import setup_logger_kwargs\n",
    "    logger_kwargs = setup_logger_kwargs(exp_name, seed)\n",
    "    # from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "    env_fn = lambda : create_train_env(1,1,'complex')\n",
    "    # env_fn = SubprocVecEnv([])\n",
    "    # env_fn = lambda : JoypadSpace(gym_super_mario_bros.make(\"SuperMarioBros-{}-{}-v0\".format(1, 1)), gym_super_mario_bros.actions.COMPLEX_MOVEMENT)\n",
    "    ppo(env_fn, actor=userActor, critic=userCritic,#core.MLPActorCritic, #gym.make(args.env)\n",
    "        ac_kwargs=dict(hidden_sizes=hid_sizes), gamma=gamma,\n",
    "        seed=seed, steps_per_epoch=steps, epochs=epochs,\n",
    "        logger_kwargs=logger_kwargs, clip_ratio=0.2, pi_lr=0.001, vf_lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.查看训练结果\n",
    "!pwd\n",
    "%matplotlib inline\n",
    "!python -m spinup.run plot /root/lele/spinningup/spinningup/data/ppo/ppo_s0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4.2.1 设计DL模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关包\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义通用cnn model作为base model\n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, num_inputs, num_out, activation=nn.ReLU): \n",
    "        super(cnn_model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1) #卷积层\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)  #卷积层\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)  #卷积层\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)  #卷积层\n",
    "        self.liner = nn.Linear(32 * 6 * 6, 512) #线性层\n",
    "        self.fc_out = nn.Linear(512, num_out)   #输出层\n",
    "        self._initialize_weights() #模型权重初始化\n",
    "\n",
    "    #对模型参数进行初始化，合理的初始化对训练的收敛起到非常好的作用\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.LSTMCell):\n",
    "                nn.init.constant_(module.bias_ih, 0)\n",
    "                nn.init.constant_(module.bias_hh, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1) #把卷积输出的高纬数据拉平\n",
    "        x = F.relu(self.liner(x))\n",
    "        out = self.fc_out(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4.2.2 设计ppo的ac模型（Actor/Critic）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor负责policy，输出具体要执行的action， Critic负责预测该状态下的value 指引action更新policy \n",
    "训练的时候我们通过Critic的指引来更新Actor，而Critic是由Reward指引更新的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义actor，负责输出action的概率分布，对应简化版的pi函数\n",
    "class userActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = cnn_model(obs_dim, act_dim, activation=activation) #定义策略模型，输出为动作空间大小\n",
    "        print(self.logits_net)\n",
    "\n",
    "    #计算策略分布，和action的概率\n",
    "    def forward(self, obs, act=None):\n",
    "        pi = Categorical(logits=self.logits_net(obs))\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = pi.log_prob(act)\n",
    "        return pi, logp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义Critic，对应简化板的v函数，训练的时候我们通过Critic的指引来更新Actor，而Critic是由Reward指引更新的\n",
    "class userCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = cnn_model(obs_dim, 1, activation=activation) #输出为1，因为输出为当前策略的value预测值（一维）\n",
    "        print(self.v_net)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4.2.3 定义ppo 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efe46b6a5583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import scipy.signal\n",
    "#from core import userCritic, userActor\n",
    "from env import create_train_env\n",
    "from spinup.utils.logx import EpochLogger\n",
    "from spinup.utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定使用gpu\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils:\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R4.2.3.1 定义PPOBuffer 用来存储交互数据，提供给模型训练使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "\n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self): #取数据用于训练\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        \n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32).to(device) for k,v in data.items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R4.2.3.2 定义ppo算法 及 更新策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义policy模型\n",
    "ac_pi = actor(4, act_dim, activation=nn.Tanh)  # 输入为观察值的channel（4）输出为action的深度（7）激活函数使用Tanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义value模型\n",
    "ac_v = critic(4, activation=nn.Tanh)  # 输入为观察值的channel（4）激活函数使用Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing PPO policy loss\n",
    "def compute_loss_pi(data):\n",
    "    obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "    # Policy loss\n",
    "    pi, logp = ac_pi(obs, act) #计算action的概率分布\n",
    "    ratio = torch.exp(logp - logp_old) #计算新老策略的差异大小\n",
    "    clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv #clip 新老策略的更新范围在（1-clip_ratio, 1+clip_ratio）内\n",
    "    loss_pi = -(torch.min(ratio * adv, clip_adv)).mean() #计算最终的policy loss 使adv（优势）更明显的方向做梯度更新\n",
    "\n",
    "    return loss_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing value loss\n",
    "def compute_loss_v(data):\n",
    "    obs, ret = data['obs'], data['ret']\n",
    "    return ((ac_v(obs) - ret)**2).mean() #真实值和预测值做均方差loss 使v模型预测更接近真实值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizers for policy and value function\n",
    "pi_optimizer = Adam(ac_pi.parameters(), lr=pi_lr)\n",
    "vf_optimizer = Adam(ac_v.parameters(), lr=vf_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update 模型参数（训练）\n",
    "def update():\n",
    "    data = buf.get() #读取训练数据\n",
    "\n",
    "    # Train policy with multiple steps of gradient descent\n",
    "    for i in range(train_pi_iters):\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi, pi_info = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "    # Value function learning\n",
    "    for i in range(train_v_iters):\n",
    "        vf_optimizer.zero_grad()\n",
    "        loss_v = compute_loss_v(data)\n",
    "        loss_v.backward()\n",
    "        vf_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for interaction with environment\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for epoch in range(1000):\n",
    "    for t in range(2000):\n",
    "        with torch.no_grad(): #收集数据过程，不做参数更新\n",
    "            rr = torch.from_numpy(o.copy()).float().to(device) #数据转换\n",
    "            pi, _ = ac_pi(rr, None) # 计算pi\n",
    "            a = pi.sample() #取出action用于环境交互\n",
    "            logp = pi.log_prob(a) #去除a对应的概率存起来用于模型训练时判断更新幅度，防止步幅过大学习率过大\n",
    "            v = ac_v(torch.as_tensor(o, dtype=torch.float32).to(device)) # 计算v存起来给pi模型提供指引\n",
    "\n",
    "        next_o, r, d, _ = env.step(a.cpu().numpy().item()) #在游戏中执行模型输出的action\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # save and log\n",
    "        buf.store(o, a.cpu().numpy(), r, v.cpu().numpy(), logp.cpu().numpy())\n",
    "\n",
    "        # Update obs (critical!)\n",
    "        o = next_o\n",
    "\n",
    "        #下面部分为对回合结束时的特殊情况做一下处理，比如最大步数达到了但是并没有gameover则需要获取下一帧的观察值，而小人game over时则不需要，初学者可以掠过\n",
    "        timeout = ep_len == max_ep_len\n",
    "        terminal = d \n",
    "        epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "        if terminal or epoch_ended:\n",
    "            if epoch_ended and not(terminal):\n",
    "                print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "            # if trajectory didn't reach terminal state, bootstrap value target\n",
    "            if epoch_ended:\n",
    "                print('epoch_end')\n",
    "                with torch.no_grad():\n",
    "                    v =ac_v(torch.from_numpy(o).float().to(device)).cpu().numpy()\n",
    "            else:\n",
    "                print('epret :',ep_ret)\n",
    "                v = 0\n",
    "            buf.finish_path(v)\n",
    "            if terminal:\n",
    "                # only save EpRet / EpLen if trajectory finished\n",
    "                logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Perform PPO update!\n",
    "    update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R4.2.3.3 PPO算法完整代码（添加log记录、mpi多进程）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo函数完整代码\n",
    "def ppo(env_fn, actor=nn.Module, critic=nn.Module, ac_kwargs=dict(), seed=0,\n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        target_kl=0.01, logger_kwargs=dict(), save_freq=10):\n",
    "    # Special function to avoid certain slowdowns from PyTorch + MPI combo.\n",
    "    setup_pytorch_for_mpi()\n",
    "\n",
    "    # Set up logger and save configuration\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    # Random seed\n",
    "    seed += 10000 * proc_id()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.n\n",
    "    \n",
    "    # Create actor-critic module\n",
    "    ac_pi = actor(obs_dim[0], act_dim, hidden_sizes=[64, 64], activation=nn.Tanh)  # env.observation_space, env.action_space, nn.ReLU)\n",
    "    ac_v = critic(obs_dim[0], hidden_sizes=[64, 64], activation=nn.Tanh)  # env.observation_space, nn.ReLU)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cup')\n",
    "    ac_pi.to(device)\n",
    "    ac_v.to(device)\n",
    "\n",
    "    # Sync params across processes\n",
    "    sync_params(ac_pi)\n",
    "    sync_params(ac_v)\n",
    "\n",
    "    # Count variables\n",
    "    def count_vars(module):\n",
    "        return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "    var_counts = tuple(count_vars(module) for module in [ac_pi, ac_v])\n",
    "    logger.log('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "    # Set up experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch / num_procs())\n",
    "    buf = PPOBuffer(obs_dim, env.action_space.shape, local_steps_per_epoch, gamma, lam)\n",
    "    \n",
    "    # Set up function for computing PPO policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac_pi(obs, act)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+clip_ratio) | ratio.lt(1-clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((ac_v(obs) - ret)**2).mean()\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = Adam(ac_pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(ac_v.parameters(), lr=vf_lr)\n",
    "\n",
    "    # Set up model saving\n",
    "    logger.setup_pytorch_saver(ac_pi)\n",
    "    \n",
    "    def update():\n",
    "        data = buf.get()\n",
    "\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = compute_loss_pi(data)\n",
    "            kl = mpi_avg(pi_info['kl'])\n",
    "            if kl > 1.5 * target_kl:\n",
    "                logger.log('Early stopping at step %d due to reaching max kl.'%i)\n",
    "                break\n",
    "            loss_pi.backward()\n",
    "            mpi_avg_grads(ac_pi)    # average grads across MPI processes\n",
    "            pi_optimizer.step()\n",
    "\n",
    "        logger.store(StopIter=i)\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            mpi_avg_grads(ac_v)    # average grads across MPI processes\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        # Log changes from update\n",
    "        kl, ent, cf = pi_info['kl'], pi_info_old['ent'], pi_info['cf']\n",
    "        logger.store(LossPi=pi_l_old, LossV=v_l_old,\n",
    "                     KL=kl, Entropy=ent, ClipFrac=cf,\n",
    "                     DeltaLossPi=(loss_pi.item() - pi_l_old),\n",
    "                     DeltaLossV=(loss_v.item() - v_l_old))\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            # a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "            with torch.no_grad():\n",
    "                rr = torch.from_numpy(o.copy()).float().to(device)#.unsqueeze(0)\n",
    "                pi, _ = ac_pi(rr, None)\n",
    "                a = pi.sample()\n",
    "                # logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "                logp = pi.log_prob(a)#.sum(axis=-1)\n",
    "                v = ac_v(torch.as_tensor(o, dtype=torch.float32).to(device))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a.cpu().numpy().item())\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a.cpu().numpy(), r, v.cpu().numpy(), logp.cpu().numpy())\n",
    "            logger.store(VVals=v.cpu().numpy())\n",
    "\n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d #or timeout\n",
    "            epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if epoch_ended:\n",
    "                    print('epoch_end')\n",
    "                    # _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                    with torch.no_grad():\n",
    "                        v =ac_v(torch.from_numpy(o).float().to(device)).cpu().numpy()\n",
    "                else:\n",
    "                    print('epret :',ep_ret)\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Perform PPO update!\n",
    "        update()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('VVals', with_min_and_max=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', (epoch+1)*steps_per_epoch)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossV', average_only=True)\n",
    "        logger.log_tabular('DeltaLossPi', average_only=True)\n",
    "        logger.log_tabular('DeltaLossV', average_only=True)\n",
    "        logger.log_tabular('Entropy', average_only=True)\n",
    "        logger.log_tabular('KL', average_only=True)\n",
    "        logger.log_tabular('ClipFrac', average_only=True)\n",
    "        logger.log_tabular('StopIter', average_only=True)\n",
    "        logger.log_tabular('Time', time.time()-start_time)\n",
    "        logger.dump_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R4.2.3.3 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpi_fork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-77f82f0b8df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KMP_DUPLICATE_LIB_OK'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmpi_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run parallel code with mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mspinup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_logger_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mpi_fork' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    hid_sizes = 128\n",
    "    gamma = 0.999\n",
    "    seed = 0\n",
    "    steps = 10000\n",
    "    epochs = 150\n",
    "    cpu = 1\n",
    "    exp_name = \"ppo\"\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "    mpi_fork(cpu)  # run parallel code with mpi\n",
    "\n",
    "    from spinup.utils.run_utils import setup_logger_kwargs\n",
    "    logger_kwargs = setup_logger_kwargs(exp_name, seed)\n",
    "    env_fn = lambda : create_train_env(1,1,'complex')\n",
    "    ppo(env_fn, actor=userActor, critic=userCritic,\n",
    "        ac_kwargs=dict(hidden_sizes=hid_sizes), gamma=gamma,\n",
    "        seed=seed, steps_per_epoch=steps, epochs=epochs,\n",
    "        logger_kwargs=logger_kwargs, clip_ratio=0.2, pi_lr=0.001, vf_lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R4.2.3.4 查看训练过程指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看训练plot出来\n",
    "!pwd\n",
    "%matplotlib inline\n",
    "!python -m spinup.run plot /root/lele/spinningup/spinningup/data/ppo/ppo_s0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R4.2.3.5 加载训练好的模型并在游戏中运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cup')\n",
    "def load_pytorch_policy(fpath, itr='', deterministic=False):\n",
    "    \"\"\" Load a pytorch policy saved with Spinning Up Logger.\"\"\"\n",
    "\n",
    "    fname = osp.join(fpath, 'pyt_save', 'model' + itr + '.pt')\n",
    "    print('\\n\\nLoading from %s.\\n\\n' % fname)\n",
    "\n",
    "    model = torch.load(fname) #加载训练好的模型\n",
    "\n",
    "    # make function for producing an action given a single state\n",
    "    def get_action(x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.as_tensor(x, dtype=torch.float32)\n",
    "            pi, _ = model(x.to(device), None)\n",
    "            action = pi.sample()\n",
    "        return action.cpu()\n",
    "\n",
    "    return get_action\n",
    "\n",
    "\n",
    "def run_policy(env, get_action, max_ep_len=None, num_episodes=100, render=True):\n",
    "    assert env is not None, \\\n",
    "        \"Environment not found!\\n\\n It looks like the environment wasn't saved, \" + \\\n",
    "        \"and we can't run the agent in it. :( \\n\\n Check out the readthedocs \" + \\\n",
    "        \"page on Experiment Outputs for how to handle this situation.\"\n",
    "\n",
    "    logger = EpochLogger()\n",
    "\n",
    "    o, r, d, ep_ret, ep_len, n = env.reset(), 0, False, 0, 0, 0\n",
    "    while n < num_episodes:\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(1e-3)\n",
    "\n",
    "        a = get_action(o)\n",
    "        o, r, d, _ = env.step(a.numpy().item())\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            print('Episode %d \\t EpRet %.3f \\t EpLen %d' % (n, ep_ret, ep_len))\n",
    "            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "            n += 1\n",
    "\n",
    "    logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "    logger.log_tabular('EpLen', average_only=True)\n",
    "    logger.dump_tabular()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #这里根据你自己的spriningup安装路径来修改\n",
    "    fpath = r'/root/lele/spinningup/spinningup/data/ppo/ppo_s0/'\n",
    "    episodes = 100\n",
    "    store_true = False\n",
    "    \n",
    "    env = create_train_env(1,1, 'complx')\n",
    "    get_action = load_pytorch_policy(fpath)#itr='_50'\n",
    "    run_policy(env, get_action, 0, episodes, store_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "注：完整代码见：https://github.com/gaoxiaos/Supermariobros-PPO-pytorch.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R5 强化学习的近况&挑战\n",
    "### R5.1强化学习的近况\n",
    "强化学习一直被学术届认为是通往通用智能的大门，所以在这个领域深耕的学术论文每年都在指数增加，特别是今年各大AI会议的论坛都把强化学习的讨论放在了重要位置，比如世界人工智能大会的主论坛、ijcai今年在清华平台举办的麻将AI大赛，nips更是把四个赛道全部放在了强化学习领域\n",
    "强化学习面临的问题还有很多，比如数据采样的难度，由于数据来源于交互所以模型的学习速度依赖于采样速度，如何在有限的交互步数下取得更好的成绩就成了业内模型创新的方向。再如很多场景无法明确提出奖励函数，这时候如何让模型模仿专家达到专业的程度等各方面的研究都在进行，以及算法方面，自从ppo出来并一统江湖后已经很长时间没有出现质的飞跃的算法，这块的研究也是非常值得进一步探索的。如果你也有兴趣，那么欢迎添加微信入群，和其他小伙伴一起交流探索，打比赛拿奖金。\n",
    "### R5.2强化学习的挑战\n",
    "通过上面的学习，我们来挑战下有一定难度的新游戏“大鱼吃小鱼”-该游戏是openai新推出的一个“随机生成”环境procgen里非常有代表性的一个场景，为了解决网络模型通常的“记住”怎么走而非完全理解怎么走的问题，openai推出了procgen benchmark用来评估模型的泛化性能，每次reset游戏时游戏的分布都是随机生成的，比如大鱼吃小鱼，每次开场的小鱼分布都是随机的，小鱼行为也都是随机的，这样就需要你控制的angent要真实理解周围环境才能作出正确判断吃到更多小鱼。\n",
    "竞赛直达："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}