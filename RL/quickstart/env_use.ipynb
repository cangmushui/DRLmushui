{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这一节介绍env的基本使用\r\n",
    "env的基本方法\r\n",
    "step,reset,render,close,seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01746862, -0.01032063, -0.02640114, -0.02186856])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#了解一个env的状态空间和动作空间\r\n",
    "import gym\r\n",
    "env = gym.make('CartPole-v0')\r\n",
    "print(\"env.action_space: \", env.action_space)\r\n",
    "#env.action_space:  Discrete(2)\r\n",
    "print(\"env.observation_space: \", env.observation_space)\r\n",
    "#env.observation_space:  Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\r\n",
    "print (env.observation_space.high ) \r\n",
    "#[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\r\n",
    "print (env.observation_space.low ) \r\n",
    "#[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\r\n",
    "env.reset()\r\n",
    "#array([ 0.01746862, -0.01032063, -0.02640114, -0.02186856])\r\n",
    "# 这四个数字组成的状态变量（state variables）分别含义如下：\r\n",
    "# 0.03749292： 小车在轨道上的位置（position of the cart on the track）\r\n",
    "# -0.03226631： 杆子与竖直方向的夹角（angle of the pole with the vertical）\r\n",
    "# 0.01609263： 小车速度（cart velocity）\r\n",
    "# -0.04661368： 角度变化率（rate of change of the angle）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义空间\r\n",
    "import gym\r\n",
    "observation_space = gym.spaces.Box(low=0, high=255, shape=(4, 84, 84))\r\n",
    "action_space=gym.spaces.Discrete(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym最基本的使用\r\n",
    "import gym\r\n",
    "env = gym.make('CartPole-v0')\r\n",
    "env.reset()\r\n",
    "#执行一个向左的操作\r\n",
    "obj, reward, done, info = env.step(1) #1 向右 0向左\r\n",
    "print(\"obj\", obj)\r\n",
    "print(\"reward\", reward)\r\n",
    "print(\"done\", done)\r\n",
    "print(\"info\", info)\r\n",
    "\r\n",
    "for _ in range(1000):\r\n",
    "    env.render() \r\n",
    "    #随机获取一个动作进行执行\r\n",
    "    obj, reward, done, info = env.step(env.action_space.sample()) # take a random action\r\n",
    "    if done:\r\n",
    "        env.reset()\r\n",
    "env.close()\r\n",
    "\r\n",
    "# obj [-0.02268437  0.60981794 -0.04339413 -0.87715228]\r\n",
    "# reward 1.0\r\n",
    "# done False\r\n",
    "# info {}\r\n",
    "#一个动作执行后，环境会返回四个变量（obj:新的状态（对照前面环境初始化的状态理解）、reward：指定该动作获得的奖励值（在游戏中的得分）、\r\n",
    "#                                 done:回合是否结束（你控制的小人是不是死了，对应回合结束）、info:额外信息（该游戏较简单，info为空））"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用gym自带的录像功能,注意录像是以episode为单位的\r\n",
    "import gym\r\n",
    "env = gym.make('CartPole-v0')\r\n",
    "from gym import wrappers\r\n",
    "env = wrappers.Monitor(env,\"./gym-outputs\", force=True)\r\n",
    "env.reset()\r\n",
    "for _ in range(1000):\r\n",
    "    # env.render() \r\n",
    "    obj, reward, done, info = env.step(env.action_space.sample()) # take a random action\r\n",
    "    if done:\r\n",
    "        break\r\n",
    "env.close()\r\n",
    "\r\n",
    "# ##在notebook中关闭环境后，可以网页播放生成的mp4文件\r\n",
    "# import io\r\n",
    "# import base64\r\n",
    "# from IPython.display import HTML\r\n",
    "# video = io.open('./gym-outputs/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\r\n",
    "# encoded = base64.b64encode(video)\r\n",
    "# HTML(data='''\r\n",
    "#     <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\r\n",
    "# .format(encoded.decode('ascii')))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这一节使用wrapper对gym环境进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先引入下相关包\r\n",
    "import gym_super_mario_bros\r\n",
    "from gym.spaces import Box\r\n",
    "from gym import Wrapper\r\n",
    "from nes_py.wrappers import JoypadSpace#BinarySpaceToDiscreteSpaceEnv\r\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB图像转灰度图\r\n",
    "#借助cv2即（opencv）包快速转换COLOR_RGB2GRAY\r\n",
    "def process_frame(frame):\r\n",
    "    if frame is not None:\r\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) #图像转换\r\n",
    "        frame = cv2.resize(frame, (84, 84))[None, :, :] / 255. #裁剪合适大小，并归一化\r\n",
    "        return frame\r\n",
    "    else:\r\n",
    "        return np.zeros((1, 84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写一个继承Wrapper的包装类，一定注意在构造方法中调用父类的构造函数\r\n",
    "#1.包装类copy了原环境的以下信息,在新的类初始化时要进行相应的修改,虽然这些量\r\n",
    "#  在实现新的环境逻辑时不一定用得到;此外在包装类初始化时还要定义一些别的要\r\n",
    "#  使用的变量\r\n",
    "'''self.env = env\r\n",
    "   self.action_space = self.env.action_space\r\n",
    "   self.observation_space = self.env.observation_space\r\n",
    "   self.reward_range = self.env.reward_range\r\n",
    "   self.metadata = self.env.metadata'''\r\n",
    "#2.同时新的环境逻辑通过重写step和reset方法实现,只能重写step和reset\r\n",
    "\r\n",
    "class CustomReward(Wrapper):\r\n",
    "  '''这个类的作用\r\n",
    "  1.处理状态空间,将RGB转为灰度,并将图像裁剪为84x84\r\n",
    "  2.设定新的奖励函数\r\n",
    "  这里我们做了几个小优化如下：\r\n",
    "      1).reward += (info[\"score\"] - self.curr_score) / 40.\r\n",
    "      原来的reward仅包含了对“离终点更近”的奖励和“时间消耗”、”死掉“的惩罚\r\n",
    "      为了让游戏更好玩，我们添加了info[\"score\"]，包含了对获得技能、金币的\r\n",
    "      奖励，但不是重点，为了不影响整体要通关的属性，弱化他\r\n",
    "      2).我们对回合结束时到达终点和未达到的奖励和惩罚进行放大，激励agent\r\n",
    "      更快速的到达终点\r\n",
    "      if done:\r\n",
    "                  if info[\"flag_get\"]:\r\n",
    "                      reward += 50\r\n",
    "                  else:\r\n",
    "                      reward -= 50\r\n",
    "      \r\n",
    "      3.这里仅仅是对reward修改的一些示例，后面自己在实战时可以自己根据实际\r\n",
    "      情况进行定义，比如当agent有时陷入一个错误的路线卡住时，可以添加一个缓\r\n",
    "      冲区让agent学会后退等\r\n",
    "  '''\r\n",
    "\r\n",
    "  def __init__(self, env=None):\r\n",
    "      super().__init__(env)\r\n",
    "      self.observation_space= Box(low=0,high=255,shape=(1,84,84))\r\n",
    "      self.curr_score = 0\r\n",
    "\r\n",
    "  # 重写step方法以处理状态空间并规定新的奖励函数\r\n",
    "  def step(self,action):\r\n",
    "      # 走一步,拿到原有的奖励\r\n",
    "      state,reward,done,info=self.env.step(action)\r\n",
    "      state=process_frame(state)\r\n",
    "      reward += (info[\"score\"]-self.curr_score)/40.\r\n",
    "      self.curr_score = info[\"score\"]\r\n",
    "      if done:\r\n",
    "          if info[\"flag_get\"]:\r\n",
    "              reward += 50\r\n",
    "          else:\r\n",
    "              reward -= 50\r\n",
    "      return state, reward / 10., done, info\r\n",
    "  #reset需要初始化一些自定义变量并返回一个初始状态\r\n",
    "  def reset(self):\r\n",
    "    self.curr_score = 0\r\n",
    "    return process_frame(self.env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在学习时并不需要所有帧,我们可以连续4帧给相同的输入,并将结果合并为一帧\r\n",
    "class CustomSkipFrame(Wrapper):\r\n",
    "    def __init__(self, env,skip=4) -> None:\r\n",
    "        super().__init__(env)\r\n",
    "        self.observation_space = Box(low=0, high=255, shape=(4, 84, 84))\r\n",
    "        self.skip = skip\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        total_reward = 0\r\n",
    "        states = []\r\n",
    "        state, reward, done, info = self.env.step(action)\r\n",
    "        for i in range(self.skip):\r\n",
    "            if not done:\r\n",
    "                state, reward, done, info = self.env.step(action)\r\n",
    "                total_reward += reward\r\n",
    "                states.append(state)\r\n",
    "            else:\r\n",
    "                states.append(state)\r\n",
    "        states = np.concatenate(states, 0)[None, :, :, :]\r\n",
    "        return states.astype(np.float32), reward, done, info  \r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        state = self.env.reset()\r\n",
    "        states = np.concatenate([state for _ in range(self.skip)], 0)[None, :, :, :]\r\n",
    "        return states.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#至此，我们完成了超级玛丽环境的自定义，封装如下：\r\n",
    "def create_train_env(world, stage, action_type, output_path=None):\r\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-{}-{}-v0\".format(world, stage))\r\n",
    "    if action_type == \"right\":\r\n",
    "        actions = RIGHT_ONLY\r\n",
    "    elif action_type == \"simple\":\r\n",
    "        actions = SIMPLE_MOVEMENT\r\n",
    "    else:\r\n",
    "        actions = COMPLEX_MOVEMENT\r\n",
    "    env = JoypadSpace(env, actions)\r\n",
    "    env = CustomReward(env)\r\n",
    "    env = CustomSkipFrame(env)\r\n",
    "    return env, env.observation_space.shape[0], len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试一下\r\n",
    "custom_env = create_train_env(1,1,'simple')\r\n",
    "print(custom_env)\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}